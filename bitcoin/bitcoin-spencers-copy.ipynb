{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "LABEL_TO_INDEX = {-1: 0, 0: 1, 1: 2}\n",
    "\n",
    "# Parameters\n",
    "d = 500  # recommendation is < log3(T / 100), where T is number of available time units\n",
    "theta = 0.0\n",
    "confidence = 0.5\n",
    "startTrain = '2016-01-01'\n",
    "endTrain = '2016-01-10'\n",
    "startTest = '2016-01-01'\n",
    "endTest = '2016-01-10'\n",
    "\n",
    "def loadData():\n",
    "    df = pd.read_csv('btcnCNY_1-min_data_2012-01-01_to_2017-05-31.csv', usecols=[0,4])\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    return df\n",
    "\n",
    "def normalize(x):\n",
    "    if x.Close > theta:\n",
    "        return 1\n",
    "    elif x.Close < -theta:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "# feature vector [last price diff, count of -1, 0, 1, longest consecutive -1, 0, 1]\n",
    "def extractFeaturesAndLabels(prices):\n",
    "    # history of price differences\n",
    "    diff = prices.diff()\n",
    "    h = diff.apply(normalize,'columns') \n",
    "\n",
    "    numSamples = len(h)-d\n",
    "\n",
    "    X = []\n",
    "    for i in range(1, numSamples):\n",
    "        X.append(diff[i:i+d].values.flatten().tolist())\n",
    "        \n",
    "    return X, h[d+1:].values\n",
    "\n",
    "def buildConsecutiveRunRow(label, run):\n",
    "    ret = [0, 0, 0]\n",
    "    ret[LABEL_TO_INDEX[label]] = run\n",
    "    return ret\n",
    "\n",
    "def scoreIgnoreZeros(X, y):    \n",
    "    y_predict = clf.predict_proba(X)\n",
    "    total,correct = 0,0\n",
    "    for i,yp in enumerate(y_predict):\n",
    "        if yp[0] > confidence or yp[2] > confidence:\n",
    "            # -1,1 > than confidence threshold. classes are [-1, 0, 1]\n",
    "            p = -1 if yp[0] > confidence else 1\n",
    "            correct += 1 if p==y[i] else 0\n",
    "            total += 1\n",
    "    return correct, total\n",
    "\n",
    "def scoreNoIgnore(X, y):    \n",
    "    y_predict = clf.predict(X) \n",
    "    total = len(y)\n",
    "    correct = 0\n",
    "    for i in range(len(y)):\n",
    "        actual = y[i]\n",
    "        predicted = y_predict[i]\n",
    "        if actual == predicted:\n",
    "            correct += 1\n",
    "    return correct, total\n",
    "\n",
    "# converts an array of quantized values (ex. [-1, 0, 1, 1, -1]) to a number\n",
    "def toIndex(a):\n",
    "    acc = 0\n",
    "    for x in a:\n",
    "        acc = acc*3 + (x+1) # shift quantized values by 1\n",
    "    return acc\n",
    "    \n",
    "def buildProbabilityMap(prices):\n",
    "    q = prices.diff().apply(normalize, 'columns')\n",
    "    ec = [[0]*3 for _ in range(3**d)]\n",
    "    for e in range(d, len(q)):\n",
    "        n = toIndex(q[e-d:e])\n",
    "        ec[n][q[e]+1] += 1\n",
    "    return ec\n",
    "\n",
    "def predictEC(n, ec):\n",
    "    total = sum(ec[n])\n",
    "    if total==0: return 0\n",
    "    if ec[n][0] / total > confidence:\n",
    "        return -1\n",
    "    elif ec[n][2] / total > confidence:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def scoreEC(prices, ec):\n",
    "    q = prices.diff().apply(normalize, 'columns')\n",
    "    correct = total = 0\n",
    "    for e in range(d, len(q)):\n",
    "        n = toIndex(q[e-d:e])\n",
    "        yp = predictEC(n, ec)\n",
    "        if yp != 0:\n",
    "            total += 1\n",
    "            correct += 1 if yp==q[e] else 0\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## Load and preprocess data ##############\n",
    "df = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Extract features and labels ############\n",
    "X_train,y_train = extractFeaturesAndLabels(df[startTrain:endTrain])\n",
    "\n",
    "X_test, y_test = extractFeaturesAndLabels(df[startTest:endTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ Add Polynomial features #################\n",
    "# poly = PolynomialFeatures(2, interaction_only = True)\n",
    "# X_train_poly = poly.fit_transform(X_train)\n",
    "# X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[startTrain:endTrain].diff().values[0:5].flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "# diff = df[startTrain:endTrain].diff()\n",
    "# h = diff.apply(normalize,'columns')\n",
    "# print(h[0:10])\n",
    "# print(diff[0:10])\n",
    "print(X_train[0:10])\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy:  0.5977565368512521\n",
      "# Trades:  12659\n",
      "clf.score:  0.588315706166\n"
     ]
    }
   ],
   "source": [
    "################## Train Model ############################\n",
    "#clf = RandomForestClassifier()\n",
    "clf = LogisticRegression(multi_class= 'multinomial', solver='newton-cg')\n",
    "#clf = DummyClassifier()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "correct, total = scoreIgnoreZeros(X_test, y_test)\n",
    "# correctNoIgnore, totalNoIgnore = scoreNoIgnore(X_test, y_test)\n",
    "\n",
    "print(clf)\n",
    "print(\"Accuracy: \",correct/total if total != 0 else 0)\n",
    "print(\"# Trades: \", total)\n",
    "print(\"clf.score: \", clf.score(X_test, y_test))\n",
    "\n",
    "# print(\"Accuracy (no ignore): \",correctNoIgnore/totalNoIgnore)\n",
    "# y_predict = clf.predict(X_train)\n",
    "# for i in range(500):\n",
    "#     print(y_predict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "################# Use the EC Model #######################\n",
    "# Doesn't give any better results\n",
    "# ec = buildProbabilityMap(df[startTrain:endTrain])\n",
    "\n",
    "# corr, total = scoreEC(df[startTest:endTest], ec)\n",
    "# print(corr, total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
